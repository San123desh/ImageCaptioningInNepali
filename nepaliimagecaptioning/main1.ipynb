{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from domain.feature_extraction import ImageFeatureExtractor\n",
    "from datasetutil.caption_processing import CaptionProcessor\n",
    "from utils.model_utils import DataHandler\n",
    "from service.evaluation import CaptionEvaluator\n",
    "from service.train import Trainer\n",
    "from domain.model_definition import ImageCaptionModel\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "# Directories\n",
    "BASE_DIR = 'Flickr8k_Dataset'\n",
    "WORKING_DIR = 'working'\n",
    "\n",
    "# Load and preprocess captions\n",
    "data_handler = DataHandler()\n",
    "captions_path = os.path.join(BASE_DIR, 'captions.txt')\n",
    "captions_doc = data_handler.load_captions(captions_path)\n",
    "\n",
    "caption_processor = CaptionProcessor()\n",
    "mapping = caption_processor.create_mapping(captions_doc)\n",
    "caption_processor.clean_mapping(mapping)\n",
    "\n",
    "# Prepare tokenizer and dataset\n",
    "all_captions = [caption for captions in mapping.values() for caption in captions]\n",
    "tokenizer = caption_processor.create_tokenizer(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = caption_processor.get_max_length(all_captions)\n",
    "\n",
    "# Validate special tokens\n",
    "assert \"startseq\" in tokenizer.word_index, \"Error: 'startseq' missing in tokenizer.\"\n",
    "assert \"endseq\" in tokenizer.word_index, \"Error: 'endseq' missing in tokenizer.\"\n",
    "\n",
    "# Debugging Tokenizer\n",
    "print(\"Tokenizer Vocabulary Size:\", len(tokenizer.word_index))\n",
    "print(\"Sample Tokenizer Mapping (First 10):\", {k: tokenizer.word_index[k] for k in list(tokenizer.word_index)[:10]})\n",
    "\n",
    "# Split Dataset\n",
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.70)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]\n",
    "\n",
    "# Load InceptionV3 model and extract features\n",
    "feature_extractor = ImageFeatureExtractor()\n",
    "features = feature_extractor.extract_features(os.path.join(BASE_DIR, 'Images'))\n",
    "\n",
    "# Save features for later use\n",
    "data_handler.save_features(features, os.path.join(WORKING_DIR, 'features.pkl'))\n",
    "\n",
    "# Debugging Missing Keys\n",
    "missing_keys = [key for key in mapping.keys() if key not in features]\n",
    "if missing_keys:\n",
    "    print(f\"Warning: Missing feature keys (first 5): {missing_keys[:5]}\")\n",
    "else:\n",
    "    print(\"All keys are properly aligned between mapping and features.\")\n",
    "\n",
    "# Train Model\n",
    "image_caption_model = ImageCaptionModel(vocab_size, max_length)\n",
    "model = image_caption_model.get_model()\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "steps = len(train) // batch_size\n",
    "\n",
    "# Create Directories for Checkpoints and Logs\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "log_dir = os.path.join(\"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# Callbacks\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, verbose=1, min_lr=1e-4)\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath=\"checkpoints/model-{epoch:02d}.keras\", save_best_only=True, monitor='loss', mode='min', verbose=1)\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True)\n",
    "\n",
    "trainer = Trainer(model, tokenizer, max_length, vocab_size)\n",
    "generator = trainer.data_generator(train, mapping, features, batch_size)\n",
    "model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1, callbacks=[model_checkpoint_callback, tensorboard_callback, lr_scheduler])\n",
    "\n",
    "# Save the trained model\n",
    "trainer.save_model(model, os.path.join(WORKING_DIR, 'model.h5'))\n",
    "\n",
    "# Evaluate and Generate Captions\n",
    "features = data_handler.load_features(os.path.join(WORKING_DIR, 'features.pkl'))\n",
    "evaluator = CaptionEvaluator(model, tokenizer, max_length)\n",
    "\n",
    "# BLEU Scores Evaluation\n",
    "bleu_scores = evaluator.evaluate_model(test, mapping, features)\n",
    "print(\"BLEU-Scores:\", bleu_scores)\n",
    "\n",
    "# Generate Captions for Sample Images\n",
    "sample_images = [\"10815824_2997e03d76.jpg\", \"23445819_3a458716c1.jpg\", \"3703960010_1e4c922a25.jpg\", \"36422830_55c844bc2d.jpg\"]\n",
    "for image_name in sample_images:\n",
    "    evaluator.generate_caption(image_name, mapping, features, BASE_DIR)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageCaption",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
